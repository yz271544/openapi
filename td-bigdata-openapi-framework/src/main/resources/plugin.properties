#=============================#
#===== plugin properties =====#
#=============================#

# Public Parameter
plug.blank = \\u0020
plug.fieldDelimiter = \\u0002
plug.fetchPlug.fieldDelimiter = \\u0002
plug.formatPlug.fieldDelimiter = \\u0009
plug.fileDelimiter = /
plug.dbConnSign = .

plug.fileCode = UTF8
plug.fetchPlug.fileCode = UTF8
plug.fetchPlug.tdFileCode = GBK
plug.cmdFileAppend = cmd
plug.formatPlug.checkFileAppend = chk
plug.formatPlug.titleFileAppend = til


# File Path
plug.HDFSInfo.uri = hdfs://master01:8020/
plug.HDFSInfo.user = oapi

plug.useFuseFlag = false

### HDFS /全路径
plug.hdfsWorkDir = /user/oapi/data/

### Relative paths
plug.targetDir = /user/oapi/

plug.ftchPlug.targetDir = /user/oapi/data/normal/
plug.frmtPlug.sourceDir = /user/oapi/data/normal/
plug.frmtPlug.targetDir = /user/oapi/data/
plug.FtchPlug.tmpFileDir = /user/oapi/tmp/
### Local Path ( absolute paths )
plug.tmpCmdDir = test/
plug.tmpCachDir = cache/
###plug.tmpCmdDir = /data/open_api/APP/test/
###plug.tmpCachDir = /data/open_api/APP/cache/
plug.sysFuseDir = /data/open_api/hdfs/


#Database relations
plug.ftchPlug.astDatabaseSchema = ttemp
plug.CachPlug.hiveDefaultSchema = oapi
plug.CachPlug.hiveDefaultTmpSchema = oapitmp



plug.tmpTableNamePrefix = temp
plug.AstTableNameMax = 45

# Import/Export tools relations
### TDCH
    # split.by.hash|split.by.value|split.by.partition|split.by.amp
plug.CachePlug.TDCHDefaultSplitMethod = split.by.value
    # textfile | sequencefile | rcfile | orcfile
plug.CachePlug.hiveDefaultFileType = orcfile
### nCluster

### Format Jar Path
plug.formatPlug.jarPath = applib/openapi-format.jar
#plug.formatPlug.jarPath = /data/open_api/APP/applib/openapi-format.jar
plug.formatPlug.libJars = -libjars /data/open_api/APP/applib/openapi-framework.jar,/data/open_api/APP/lib/poi-2.5.1-final-20040804.jar,/data/open_api/APP/lib/scala-library-2.10.4.jar,/data/open_api/APP/lib/scala-reflect-2.10.4.jar,/data/open_api/APP/lib/jerkson_2.10-0.6.8.jar

# SQL statements Template
plug.CachPlug.columnInfoQuery = select \n field_name, sorc_field_type, field_targt_type, ppi_flag \n from opi.sorc_field_info_vw \n where source_id = %d \n and schema_name = '%s'\n and tab_name = '%s'
plug.CachPlug.hiveCreatePartTable = CREATE TABLE IF NOT EXISTS %s (\n %s \n)\n partitioned by(%s comment 'Time') \n row format delimited \n fields terminated by '\u0001' \nstored as orcfile
plug.CachPlug.hiveCreateNoPartTable = CREATE TABLE IF NOT EXISTS %s (\n %s \n) row format delimited \n fields terminated by '\u0001' \n stored as orcfile
plug.CachPlug.hiveCreateExternalTable = CREATE EXTERNAL TABLE %s (\n %s \n) row format delimited \n fields terminated by '\u0001' \n stored as textfile \n LOCATION '%s'
plug.CachPlug.hiveCreatePartition = ALTER TABLE %s ADD IF NOT EXISTS PARTITION ( %s )
plug.CachPlug.hiveDropPartition = ALTER TABLE %s DROP IF EXISTS PARTITION ( %s )
plug.dropTableTemplate = drop table if exists %s
plug.astCreateTableAsTemplate = create fact table %s as (select %s from %s where 1=1 %s)

# Default Database Source
plug.CachPlug.postgresSourceInfo = 4, 04, 4,OpenAPI元数据库, 192.168.20.1, 5432, open_api, open_api, 123456, 0, 0,0,org.postgresql.Driver,true,true,true
plug.CachPlug.hiveSourceInfo = 3,03,3,山西移动Hive数据交换中心,192.168.20.119, 10000, default, oapi, oapi, 1, 5, 6, org.apache.hive.jdbc.HiveDriver,false,true,true
# kerberos configuration
hadoop.isKrb = false
linux.krb5.conf.path = /etc/krb5.conf
hadoop.security.authentication = kerberos
dfs.namenode.kerberos.principal = hdfs/_HOST@SXMCC.COM
hdfs.kerberos.user = hdptd@SXMCC.COM
hdfs.kerberos.path = /home/hdptd/open_api/util/krb5/hdptd.keytab

#ftp
ftp.test.push.dir=/data/open_api/APP/conf/master
ftp.test.push.filename=ftpTest.txt


